import time
import datetime
import threading
from math import trunc
from dateutil import parser

from lib.fifoqueue import FIFOQueue_2
from strategies.strategy import Strategy
from serviceresolver.serviceselector import ServiceSelector

from profilers.staticprofiler import StaticProfiler
from profilers.adaptiveprofiler import AdaptiveProfiler

# Adaptive retrieval strategy
# This strategy would retrieve from the context provider only when the freshness can't be met.
# The algorithm is adaptive becasue the freshness decay gradient adapts based on the current infered lifetime.
# i.e. Steep gradient when lifetime is small and shallower gradient when lifetime is longer.
# However, this does not always refresh for the most expensive SLA either. 
# Adaptive create cache misses and potentially vulanarable to data inaccuracies.
# Therefore, a compromise between the greedy and reactive.

class Adaptive(Strategy):  
    def __init__(self, db, window, isstatic=True):
        self.__observed = {}
        self.__moving_window = window

        self.service_selector = ServiceSelector()
        if(isstatic):
            self.__profiler = AdaptiveProfiler(db, self.__moving_window, self.__class__.__name__.lower())
        else:
            self.__profiler = StaticProfiler(db, self.__moving_window, self.__class__.__name__.lower())
    
    # Init_cache initializes the cache memory. 
    def init_cache(self):
        # Set current session to profiler if not set
        if(self.__profiler.session == None):
            self.__profiler.session = self.session

        # Initializing background thread clear observations.
        thread = threading.Thread(target=self.run, args=())
        thread.daemon = True               
        thread.start() 

    def run(self):
        while True:
            self.clear_expired()
            # Observing the attributes that has not been cached within the window
            time.sleep(self.__moving_window/1000) 
    
    # Clear function that run on the background
    async def clear_expired(self) -> None:
        exp_time = datetime.datetime.now() - datetime.timedelta(milliseconds=self.__moving_window)
        for key,value in self.__observed.items():
            if(value.get_last() < exp_time):
                del self.__observed[key]
            else:
                for tstamp in value:
                    if(tstamp < exp_time):
                        value.remove(tstamp)
                    else:
                        break
    
    # Returns the current statistics from the profiler
    def get_current_profile(self):
        self.profiler.get_details()

    # Retrieving context data
    def get_result(self, json = None, fthresh = 0, session = None) -> dict:               
        refetching = [] # Freshness not met for the value generated by a producer [(entityid, prodid)]
        new_context = [] # Need to fetch the entity with all attributes [(entityid, [attributes])]
        now = datetime.datetime.now()

        output = {}
        for ent in json:
            # Check freshness of requested attributes
            entityid = ent['entityId']
            if(entityid in self.cache_memory.entityhash):
                # Entity is cached
                # Atleast one of the attributes of the entity is already cached 
                lifetimes = None
                if(isinstance(self.__profiler, StaticProfiler)):
                    lifetimes = self.service_registry.get_context_producers(entityid,ent['attributes'])
                
                # Refetch from the producer if atleast 1 of it's attributes are not available or not fresh
                if(self.cache_memory.are_all_atts_cached(entityid, ent['attributes'])):
                    # All of the attributes requested are in cache for the entity
                    for att_name in ent['attributes']:
                        # Get all values from the context producers for the attribute in cache
                        att_in_cache = self.cache_memory.get_value_by_key(entityid, att_name)
                        if(isinstance(self.__profiler, AdaptiveProfiler)):
                            for prodid,val,lastret in att_in_cache:
                                # Get the index of the cache slot in which the attribute is cached
                                idx = self.__profiler.get_lookup[str(entityid)+'.'+str(prodid)+'.'+att_name]
                                # Estimated lifetime of the attribute
                                mean_for_att = self.__profiler.get_means(idx)  
                                extime = mean_for_att * (1 - fthresh)
                                time_at_expire = lastret + datetime.timedelta(milisseconds=extime)
                                if(now > time_at_expire):
                                    # If the attribute doesn't meet the freshness level (Cache miss) from the producer
                                    # add the entity and producer to the need to refresh list.
                                    refetching.append((entityid,prodid,lifetimes[prodid]['url']))
                                    break
                        else:
                            # Checking if any of the attributes are not fresh
                            for prodid,val,lastret in att_in_cache:
                                lt = lifetimes[prodid]['lifetimes'][att_name]
                                if(lt<0):
                                    continue
                                else:
                                    extime = lt * (1 - fthresh)
                                    time_at_expire = lastret + datetime.timedelta(seconds=extime)
                                    if(now > time_at_expire):
                                        # If the attribute doesn't meet the freshness level (Cache miss) from the producer
                                        # add the entity and producer to the need to refresh list.
                                        refetching.append((entityid,ent['attributes'],prodid,lifetimes[prodid]['url']))
                                        break
                else:
                    # Atleast one of the attributes requested are not in cache for the entity
                    new_context.append((entityid,ent['attributes'],lifetimes))

                # Multithread this
                if(len(new_context)>0):
                    self.__refresh_cache_for_entity(new_context)
                if(len(refetching)>0):
                    self.__refresh_cache_for_producers(refetching)
            else:
                # Even the entity is not cached previously
                # So, first retrieving the entity
                output[entityid] = self.__retrieve_entity(ent['attributes'],lifetimes)
                # Evaluate whether to cache
                # Run this in the background
                self.__evaluate_for_caching(output[entityid])

            output[entityid] = self.cache_memory.get_values_for_entity(entityid, ent['attributes'])
                
        return output

    # Evaluate for caching
    def __evaluate_for_caching(self, entity_details:dict):
        for entityid, attrs in entity_details.items():
            # Evaluate if the entity can be cached
            is_caching = False
            if(is_caching):
                # Add to cache 
                # Remove from observed list if exists
                print("Entity Cached")
            else:
                # Update the observed list for uncached entities
                if(entityid in self.__observed):
                    self.__observed[entityid].push(datetime.datetime.now())
                else:
                    self.__observed[entityid] = FIFOQueue_2(100).push(datetime.datetime.now())

    # Retrieving context for an entity
    def __retrieve_entity(self, attribute_list: list, metadata: dict) ->  dict:
        # Retrive raw context from provider according to the entity
        return self.service_selector.get_response_for_entity(attribute_list, 
                    list(map(lambda key,value: (key,value['url']), metadata.items())))

    def __refresh_cache_for_entity(self, new_context) -> None:
        for entityid,attribute_list,metadata in new_context:       
            response = self.service_selector.get_response_for_entity(attribute_list, 
                        list(map(lambda key,value: (key,value['url']), metadata.items())))
            # Push to profiler
            self.__profiler.reactive_push({entityid,response})
            # Save items in cache
            self.cache_memory.save(entityid,response)

    # Refreshing for selected context producers
    def __refresh_cache_for_producers(self, refresh_context) -> None:
        # Retrive raw context from provider according to the provider
        for entityid,attribute_list,prodid,url in refresh_context:
            response = self.service_selector.get_response_for_entity(attribute_list,[(prodid,url)])
            # Push to profiler
            self.__profiler.reactive_push({entityid,response})
            # Save items in cache
            self.cache_memory.save(entityid,response)
